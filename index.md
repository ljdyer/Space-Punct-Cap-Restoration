<title>TEST</title>

## Authors

**Laurence Dyer¹** \| [L.J.Dyer@wlv.ac.uk](mailto:L.J.Dyer@wlv.ac.uk) / [ljdyer@gmail.com](mailto:ljdyer@gmail.com) \| [GitHub](https://github.com/ljdyer)<br>
**Anthony Hughes**¹ \| [A.J.Hughes2@wlv.ac.uk](mailto:A.J.Hughes2@wlv.ac.uk) \| [GitHub](https://github.com/anthonyhughes)<br>
**Dhwani Shah** \| [D.R.Shah2@wlv.ac.uk](mailto:D.R.Shah2@wlv.ac.uk) \| [GitHub](https://github.com/D-Shah1427)<br>
**Burcu Can** \| [B.Can@wlv.ac.uk](mailto:B.Can@wlv.ac.uk)

¹Laurence Dyer and Anthony Hughes contributed equally to this work as first authors.

## Code repositories

### Data cleaning

**[Text Data Cleaner](https://github.com/ljdyer/Text-Data-Cleaner)** <br>
Clean text data for use in machine learning and natural language processing applications

### Evaluation

**[Feature Restoration Evaluator](https://github.com/ljdyer/Feature-Restoration-Evaluator)** <br>
Quantitative and qualitative evaluation of ML models for restoration of textual features

### Models

**[Naive Bayes Space Restorer](https://github.com/ljdyer/Naive-Bayes-Space-Restorer)**<br>
Train Naive Bayes-based statistical machine learning models for restoring spaces to unsegmented sequences of input characters. (Referred to in the paper as **NB**.)

**[BiLSTM Char Feature Restorer](https://github.com/ljdyer/BiLSTM-Char-Feature-Restorer/)**<br>
Train character-level BiLSTM models for restoration of features such as spaces, punctuation, and capitalization to unformatted texts. (Referred to in the paper as **BiLSTMCharSpace**/**BiLSTMCharE2E**.)

**[CRF for Punctuation Restoration](https://github.com/anthonyhughes/crf-punctuation-restoration)**<br>
Conditional Random Fields model for a punctuation restoration task. (Referred to in the paper as **CRF**.)

**[Punctuation Restoration using Transformer Models](https://github.com/anthonyhughes/finetuning-en-punctuation-restoration)**<br>
An extension of the paper Punctuation Restoration using Transformer Models for High-and Low-Resource Languages accepted at the EMNLP workshop W-NUT 2020. (Referred to in the paper as **BERTBiLSTM**.)
